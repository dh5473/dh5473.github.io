{"componentChunkName":"component---src-templates-post-template-tsx","path":"/ml/linear-regression/","result":{"data":{"site":{"siteMetadata":{"title":"Donhyeok's Blog","description":"Python, FastAPI, AI/ML 등 개발 기술을 다루는 Donhyeok의 기술 블로그","siteUrl":"https://dh5473.github.io"}},"allMarkdownRemark":{"edges":[{"node":{"html":"<p>집값을 예측해야 한다고 해보자. 수백 개의 거래 기록이 있고, 각 집의 면적과 거래가가 담겨 있다. 이 데이터를 보고 “이 집 크기면 얼마 정도 하겠다”를 예측하려면 어떻게 해야 할까?</p>\n<p><a href=\"/ml/overview/\">지난 글에서 살펴봤듯이</a> 이 문제는 지도학습(Supervised Learning) 중 <strong>회귀(Regression)</strong> 문제다. 정답이 있는 데이터로 모델을 학습시켜, 새로운 입력에 대한 연속적인 출력값을 예측한다.</p>\n<p>이 문제에 대한 가장 단순하고 직관적인 답이 **선형 회귀(Linear Regression)**다. 단순하다고 얕볼 수 없다. 선형 회귀 안에는 ML의 핵심 개념들 — 가설 함수, 파라미터 학습, 오차 측정, 최적화 — 이 모두 녹아 있다. 복잡한 신경망도 결국 이 원리 위에서 동작한다.</p>\n<hr>\n<h2>선형 회귀란 무엇인가</h2>\n<p>선형 회귀의 아이디어는 단순하다. <strong>데이터에 가장 잘 맞는 직선(line)을 찾는 것</strong>이다.</p>\n<p>면적(x)과 집값(y) 데이터가 있을 때, 이 점들 사이를 가장 잘 관통하는 직선을 그으면 새로운 면적에 대한 가격을 예측할 수 있다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.979166666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABcSAAAXEgFnn9JSAAABWklEQVR42qVTi26DMAzk/3+zXTdIKSTk/brZWaHQTVulGVlJnHC5s50OZKVUlEpeSnO2nDOUUtBaY57n5lLKzXn/J+tyyhgmgZN4x/l0Rt/3sNY2IOccXrFEJJyXxMKiS3ST0xLRayyLhjFmA+SRrRL7dVz9a01qooIzPSY9cgRdjPEFJrX5A4jk+omUnWCdvO/fJTOg9/7A5AC1i9USMagB1Y8oJDHZ5elSAhRCbIDPjLZjJWFaBG7qAyGY7ZJkFTKtD0Xh6oUQvgFx1Wv2eLtdUNwVJbsW27PJwSF7wzKOkp2zj4rRpo8G89JDakGgYZeh2r7frON+2iRT2b0ZMFKeQPmqj0T+UTDsGSYEysWkLtQygsXiP0YMC8wyQy0S2nrKZ4Sh/uN+5NxyS/GcU8PrteFZ1dqrHOczHO9WZA7yT2z73uSneL1eMY5ji7Gvz5PnKaU25wvYPwEivK/m3M38cwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/a5df027174af7cfd2a8c81edafcc1d28/9e594/scatter-regression.avif 192w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/3124f/scatter-regression.avif 384w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/0187b/scatter-regression.avif 768w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/7799c/scatter-regression.avif 1152w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/87e54/scatter-regression.avif 1185w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/avif\"\n            /><source\n              srcset=\"/static/a5df027174af7cfd2a8c81edafcc1d28/dce26/scatter-regression.webp 192w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/955be/scatter-regression.webp 384w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/58a3c/scatter-regression.webp 768w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/ccee1/scatter-regression.webp 1152w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/aa846/scatter-regression.webp 1185w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/a5df027174af7cfd2a8c81edafcc1d28/4eff4/scatter-regression.png 192w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/0cf21/scatter-regression.png 384w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/2b7c5/scatter-regression.png 768w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/ab7b1/scatter-regression.png 1152w,\n/static/a5df027174af7cfd2a8c81edafcc1d28/9d8cf/scatter-regression.png 1185w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/a5df027174af7cfd2a8c81edafcc1d28/2b7c5/scatter-regression.png\"\n            alt=\"면적-가격 산점도와 회귀 직선\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n    </span></p>\n<p align=\"center\" style=\"color: #888; font-size: 13px;\"><em>8개 데이터 포인트에 가장 잘 맞는 선형 회귀 직선. 기울기(w)가 면적 1m²당 가격 변화량을 나타낸다.</em></p>\n<p>수학적으로 이 직선은 다음과 같이 표현한다.</p>\n<blockquote>\n<p><strong>y = wx + b</strong></p>\n<ul>\n<li><strong>y</strong>: 예측하려는 값 (집값)</li>\n<li><strong>x</strong>: 입력 특성 (면적)</li>\n<li><strong>w</strong>: 기울기(weight, 가중치) — x가 1 증가할 때 y의 변화량</li>\n<li><strong>b</strong>: 절편(bias) — x가 0일 때의 y값</li>\n</ul>\n</blockquote>\n<p>ML에서는 이 수식을 **가설 함수(Hypothesis Function)**라고 부른다. 처음에는 낯선 용어인데, 결국 “데이터를 이 수식으로 설명할 수 있다”는 가설이라는 뜻이다.</p>\n<p>그럼 **학습(Learning)**이란 무엇인가? w와 b를 데이터에 맞게 조정하는 과정이다. 처음에는 아무 직선이나 그어놓고, 데이터를 보며 점점 더 잘 맞는 직선으로 수정해나간다. 이게 모델이 “학습”한다는 말의 실제 의미다.</p>\n<hr>\n<h2>가설 함수를 코드로 표현하면</h2>\n<p>scikit-learn으로 선형 회귀를 학습시키는 건 단 몇 줄이면 된다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRegression\n\n<span class=\"token comment\"># 면적(m²)과 가격(억) 데이터</span>\narea <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">60</span><span class=\"token punctuation\">,</span> <span class=\"token number\">75</span><span class=\"token punctuation\">,</span> <span class=\"token number\">85</span><span class=\"token punctuation\">,</span> <span class=\"token number\">95</span><span class=\"token punctuation\">,</span> <span class=\"token number\">110</span><span class=\"token punctuation\">,</span> <span class=\"token number\">120</span><span class=\"token punctuation\">,</span> <span class=\"token number\">140</span><span class=\"token punctuation\">,</span> <span class=\"token number\">155</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nprice <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">2.1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2.8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3.2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3.6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4.1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4.5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5.2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5.8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>area<span class=\"token punctuation\">,</span> price<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"기울기(w): </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>model<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>    <span class=\"token comment\"># 0.0313</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"절편(b): </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>model<span class=\"token punctuation\">.</span>intercept_<span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>     <span class=\"token comment\"># 0.2087</span></code></pre></div>\n<p><code class=\"language-text\">fit()</code> 한 줄이 전부다. 내부적으로 sklearn은 데이터를 보고 오차를 가장 줄이는 w와 b를 계산한다.</p>\n<p>결과를 해석해보면: 기울기가 약 0.031이므로, 면적이 1m² 늘어날 때마다 집값이 약 310만 원 올라간다고 모델이 학습했다. 이제 새로운 면적을 넣으면 가격을 예측할 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">new_area <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">100</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\npredicted <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>new_area<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"100m² 예측 가격: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>predicted<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.2f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">억\"</span></span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 3.34억</span></code></pre></div>\n<p><code class=\"language-text\">predict()</code>가 내부적으로 하는 일은 단순하다. <code class=\"language-text\">y = 0.0313 × 100 + 0.2087 = 3.34</code>를 계산하는 것이다.</p>\n<hr>\n<h2>좋은 직선을 어떻게 고를까 — 잔차(Residual)</h2>\n<p>잠깐, 생각해보면 직선을 “데이터에 가장 잘 맞게” 그린다는 게 정확히 무슨 기준일까? 어떤 직선이 더 좋은 직선인가?</p>\n<p>기준은 **잔차(Residual)**다. 잔차는 실제값과 예측값의 차이다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">잔차 = 실제값(y) - 예측값(ŷ)</code></pre></div>\n<p>각 데이터 포인트마다 잔차가 존재한다. 아래 그림에서 수직 점선이 잔차다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.979166666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABcSAAAXEgFnn9JSAAABQ0lEQVR42pVTi26DMAzk//9yq8ajBEIgEPIA15cSShmTViMUy7qcnbskW5aF8COwrusa82EYqGkaEkKQMSbmXddR27ZxHcdxxx4jW9eFwS0VZUVFnsfNn4S1jrwPL0LvPUnZkewUd9V7V6zHCc51fIhSNFTzQG+E8zz/OcGR1Fm714yZadAjTbw3LC9MBrJ0zCtNjnGr7jROUySa+USXGkJsu3U+R9w8sAyske4HUlqT4xOBSFT1NSFcS0deN6eT61L1VBQlNXlB969vcoqnCk8Dwob5Reice3NW8URFLUiKlibzbOScJ8s4Ohl1SRi4IyYEDEL3rI/mBi0THp39b8QJNV/Sn9vnd/CSEHrh1ivWC4QwSLP4ExuCyZFjRWPggEn1lONVIQcmS8wo4E4iQBY28VHv+z4+OSnlXk8YyIFGycgHv7OvPFxZ7f8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/fce054da1a1d78f824a6a021aacd6a4b/9e594/residual-visualization.avif 192w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/3124f/residual-visualization.avif 384w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/0187b/residual-visualization.avif 768w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/7799c/residual-visualization.avif 1152w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/87e54/residual-visualization.avif 1185w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/avif\"\n            /><source\n              srcset=\"/static/fce054da1a1d78f824a6a021aacd6a4b/dce26/residual-visualization.webp 192w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/955be/residual-visualization.webp 384w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/58a3c/residual-visualization.webp 768w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/ccee1/residual-visualization.webp 1152w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/aa846/residual-visualization.webp 1185w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/fce054da1a1d78f824a6a021aacd6a4b/4eff4/residual-visualization.png 192w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/0cf21/residual-visualization.png 384w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/2b7c5/residual-visualization.png 768w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/ab7b1/residual-visualization.png 1152w,\n/static/fce054da1a1d78f824a6a021aacd6a4b/9d8cf/residual-visualization.png 1185w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/fce054da1a1d78f824a6a021aacd6a4b/2b7c5/residual-visualization.png\"\n            alt=\"잔차 시각화\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n    </span></p>\n<p align=\"center\" style=\"color: #888; font-size: 13px;\"><em>각 데이터 포인트에서 회귀 직선까지의 수직 거리가 잔차다. 빨간 점선이 잔차를 나타낸다.</em></p>\n<p>잔차가 클수록 그 데이터 포인트를 잘 못 맞추고 있다는 뜻이다. 모든 잔차를 합쳐서 최소화하는 직선이 가장 좋은 직선이다.</p>\n<p>실제 코드로 잔차를 확인해보면:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>area<span class=\"token punctuation\">)</span>\nresiduals <span class=\"token operator\">=</span> price <span class=\"token operator\">-</span> y_pred\n\n<span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>price<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>\n        <span class=\"token string-interpolation\"><span class=\"token string\">f\"면적 </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>area<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">3.0f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">m² | \"</span></span>\n        <span class=\"token string-interpolation\"><span class=\"token string\">f\"실제: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>price<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.1f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">억 | \"</span></span>\n        <span class=\"token string-interpolation\"><span class=\"token string\">f\"예측: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>y_pred<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.2f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">억 | \"</span></span>\n        <span class=\"token string-interpolation\"><span class=\"token string\">f\"잔차: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>residuals<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">+.2f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">억\"</span></span>\n    <span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">면적  60m² | 실제: 2.1억 | 예측: 2.09억 | 잔차: +0.01억\n면적  75m² | 실제: 2.8억 | 예측: 2.56억 | 잔차: +0.24억\n면적  85m² | 실제: 3.2억 | 예측: 2.87억 | 잔차: +0.33억\n면적  95m² | 실제: 3.6억 | 예측: 3.18억 | 잔차: +0.42억\n면적 110m² | 실제: 4.1억 | 예측: 3.65억 | 잔차: +0.45억\n면적 120m² | 실제: 4.5억 | 예측: 3.96억 | 잔차: +0.54억\n면적 140m² | 실제: 5.2억 | 예측: 4.59억 | 잔차: +0.61억\n면적 155m² | 실제: 5.8억 | 예측: 5.06억 | 잔차: +0.74억</code></pre></div>\n<div style=\"background: #f0f4ff; border-left: 4px solid #3182f6; padding: 16px 20px; margin: 20px 0; border-radius: 4px;\">\n  <strong>💡 참고</strong><br>\n  잔차를 단순히 합치면 양수와 음수가 상쇄되어 오차가 없는 것처럼 보일 수 있다. 그래서 실제로는 잔차를 <strong>제곱</strong>해서 더하는 잔차 제곱합(RSS)을 최소화한다. 이게 바로 다음 글에서 다룰 <strong>비용 함수(Cost Function)</strong>다.\n</div>\n<hr>\n<h2>최적 파라미터 찾기 — 정규 방정식</h2>\n<p>잔차를 최소화하는 w와 b를 어떻게 구할까?</p>\n<p>수학적으로 이 문제의 해(解)가 존재한다. **정규 방정식(Normal Equation)**이라 불리는 공식인데, 주어진 데이터에 대해 잔차 제곱합을 가장 작게 만드는 w를 직접 계산한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">w = (XᵀX)⁻¹ Xᵀy</code></pre></div>\n<p>복잡해 보이지만, “행렬 연산으로 최적의 w를 한 번에 구한다”는 뜻이다. numpy로 직접 계산해보면:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 편향 항(bias)을 추가한 입력 행렬 X 구성 — [1, x] 형태</span>\nX <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>hstack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>np<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>area<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> area<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 정규 방정식: w = (XᵀX)⁻¹ Xᵀy</span>\nw <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>linalg<span class=\"token punctuation\">.</span>inv<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">.</span>T @ X<span class=\"token punctuation\">)</span> @ X<span class=\"token punctuation\">.</span>T @ price\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"편향(b): </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>w<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>    <span class=\"token comment\"># 0.2087</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"기울기(w): </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>w<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 0.0313</span></code></pre></div>\n<p>sklearn의 <code class=\"language-text\">fit()</code>이 돌려준 것과 완전히 동일한 값이다. sklearn 내부에서 이 계산(또는 수치적으로 안정된 유사 방법)을 수행한다.</p>\n<div style=\"background: #fff3f0; border-left: 4px solid #ff6b6b; padding: 16px 20px; margin: 20px 0; border-radius: 4px;\">\n  <strong>⚠️ 주의</strong><br>\n  정규 방정식은 데이터 수(n)가 많아질수록 행렬 역행렬 계산이 O(n³)으로 느려진다. 데이터가 수십만 건 이상이면 정규 방정식보다 <strong>경사하강법(Gradient Descent)</strong>이 훨씬 효율적이다. 이 시리즈 3번 글에서 자세히 다룬다.\n</div>\n<hr>\n<h2>실전 예제: California Housing 데이터</h2>\n<p>간단한 예제를 넘어서, sklearn에 내장된 캘리포니아 주택 데이터로 end-to-end 예제를 만들어보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> fetch_california_housing\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LinearRegression\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>metrics <span class=\"token keyword\">import</span> r2_score<span class=\"token punctuation\">,</span> mean_squared_error\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\n<span class=\"token comment\"># 데이터 로드</span>\nhousing <span class=\"token operator\">=</span> fetch_california_housing<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 단순화를 위해 중위 소득(MedInc) 특성 하나만 사용</span>\nX <span class=\"token operator\">=</span> housing<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span>  <span class=\"token comment\"># MedInc: 지역의 중위 소득 (만 달러 단위)</span>\ny <span class=\"token operator\">=</span> housing<span class=\"token punctuation\">.</span>target         <span class=\"token comment\"># 중위 주택 가격 (십만 달러 단위)</span>\n\n<span class=\"token comment\"># 훈련/테스트 분리</span>\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>\n    X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> random_state<span class=\"token operator\">=</span><span class=\"token number\">42</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 모델 학습</span>\nmodel <span class=\"token operator\">=</span> LinearRegression<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 예측 및 평가</span>\ny_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"기울기(w): </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>model<span class=\"token punctuation\">.</span>coef_<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>                              <span class=\"token comment\"># 0.4185</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"절편(b):   </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>model<span class=\"token punctuation\">.</span>intercept_<span class=\"token punctuation\">:</span><span class=\"token format-spec\">.4f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>                            <span class=\"token comment\"># 0.4496</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"R² 스코어: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>r2_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.3f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>                    <span class=\"token comment\"># 0.468</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"RMSE:      </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>mean_squared_error<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> y_pred<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span><span class=\"token format-spec\">.3f</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 0.790</span></code></pre></div>\n<p>R² 스코어가 0.468이다. 중위 소득 하나만으로 집값을 예측했으니 낮지 않은 편이지만, 완벽하지는 않다. 집값에는 위치, 방 수, 건물 연도 등 수많은 요소가 영향을 미치기 때문이다. 특성을 여러 개 사용하는 **다중 선형 회귀(Multiple Linear Regression)**가 이 문제를 해결한다 — 이 시리즈 4번 글에서 다룬다.</p>\n<div style=\"background: #f8f9fa; border: 1px solid #e9ecef; padding: 20px; margin: 24px 0; border-radius: 8px;\">\n  <strong>📌 핵심 요약</strong><br><br>\n  <ul style=\"margin: 0; padding-left: 20px;\">\n    <li><code>fit()</code>: 훈련 데이터로 최적의 w, b를 학습</li>\n    <li><code>predict()</code>: y = wx + b 계산으로 예측값 반환</li>\n    <li><strong>R² 스코어</strong>: 모델이 데이터의 변동성을 얼마나 설명하는지 (0~1, 높을수록 좋음)</li>\n    <li><strong>RMSE</strong>: 평균 예측 오차의 크기 (낮을수록 좋음, 단위 동일)</li>\n  </ul>\n</div>\n<hr>\n<h2>선형 회귀의 4가지 가정</h2>\n<p>선형 회귀는 강력하지만, 데이터가 특정 조건을 만족할 때 잘 동작한다. 이 조건을 모르고 쓰다가 황당한 결과를 얻는 경우가 많다.</p>\n<table>\n<thead>\n<tr>\n<th>가정</th>\n<th>의미</th>\n<th>위반 시 증상</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>선형성</strong></td>\n<td>x와 y 사이에 선형 관계가 있어야 함</td>\n<td>잔차 플롯에서 곡선 패턴</td>\n</tr>\n<tr>\n<td><strong>독립성</strong></td>\n<td>각 데이터 포인트가 서로 독립</td>\n<td>시계열에서 자기상관 발생</td>\n</tr>\n<tr>\n<td><strong>등분산성</strong></td>\n<td>잔차의 분산이 x 범위에서 일정</td>\n<td>잔차 플롯에서 부채꼴 모양</td>\n</tr>\n<tr>\n<td><strong>정규성</strong></td>\n<td>잔차가 정규 분포를 따름</td>\n<td>Q-Q 플롯에서 이탈</td>\n</tr>\n</tbody>\n</table>\n<p>이 가정들을 **잔차 플롯(Residual Plot)**으로 빠르게 진단할 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n\ny_pred_train <span class=\"token operator\">=</span> model<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">)</span>\nresiduals <span class=\"token operator\">=</span> y_train <span class=\"token operator\">-</span> y_pred_train\n\nplt<span class=\"token punctuation\">.</span>figure<span class=\"token punctuation\">(</span>figsize<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>scatter<span class=\"token punctuation\">(</span>y_pred_train<span class=\"token punctuation\">,</span> residuals<span class=\"token punctuation\">,</span> alpha<span class=\"token operator\">=</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> s<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> color<span class=\"token operator\">=</span><span class=\"token string\">'steelblue'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>axhline<span class=\"token punctuation\">(</span>y<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> color<span class=\"token operator\">=</span><span class=\"token string\">'red'</span><span class=\"token punctuation\">,</span> linestyle<span class=\"token operator\">=</span><span class=\"token string\">'--'</span><span class=\"token punctuation\">,</span> linewidth<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>xlabel<span class=\"token punctuation\">(</span><span class=\"token string\">'예측값 (ŷ)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>ylabel<span class=\"token punctuation\">(</span><span class=\"token string\">'잔차 (y - ŷ)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">'잔차 플롯 (Residual Plot)'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>tight_layout<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>잔차가 예측값 전 범위에 걸쳐 0 주변으로 균등하게 분포하면 가정이 충족된 것이다. 패턴이 보이거나(U자, 부채꼴) 한쪽으로 치우쳐 있으면 가정이 위반된 신호다.</p>\n<div style=\"background: #f0fff4; border-left: 4px solid #51cf66; padding: 16px 20px; margin: 20px 0; border-radius: 4px;\">\n  <strong>✅ 팁</strong><br>\n  실전에서는 가정을 완벽히 만족하는 데이터가 거의 없다. 가정 위반이 심하면 변수 변환(로그 변환 등)이나 다른 모델을 고려해야 한다. 하지만 가정 위반 여부를 <strong>확인하는 습관</strong> 자체가 중요하다. 잔차 플롯은 항상 그려보자.\n</div>\n<hr>\n<h2>흔한 실수와 한계</h2>\n<h3>비선형 데이터에 억지로 직선 끼워 맞추기</h3>\n<p>가장 많이 하는 실수다. 실제 데이터가 곡선 패턴을 보이는데 선형 회귀를 적용하면, 아무리 학습해도 잘 맞지 않는다. 잔차 플롯에서 U자 또는 역U자 패턴이 보이면 선형 가정이 맞지 않는다는 신호다.</p>\n<p>이 경우 다항 회귀(Polynomial Regression)를 쓰거나 변수를 로그 변환하는 방법을 고려한다.</p>\n<h3>이상치(Outlier) 1개가 직선을 망친다</h3>\n<p>선형 회귀는 이상치에 매우 민감하다. 잔차를 제곱해서 최소화하기 때문에, 극단적인 값 하나가 전체 직선을 그쪽으로 끌어당긴다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 768px; \"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 61.979166666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABcSAAAXEgFnn9JSAAABcUlEQVR42pVTi27CMAzk/39xbNqAAW1Ymz7TpE1ud2EwNIE0LLmxrPpyPjsr0JYYEVLEMs9YlkUpBO9hmwbDMKDh2bYtrLU5lnddh3u2SinBnE44liWMMSiKAuM4ZkcISLzkGVvp0zUW5rDHx2aD9XqN7XaLnszgHNI0PQcohi1bmVnY931uTeyU/4/pv6sLMFK/iWA6H1TcB1B8j2Fd11n4RMDb2y5AD5kqT309yXR9h8/iSIUcVmrTc6K3DNONZ9Pk5RxSY0mAElmeb8UBh5NBpQ2Y3K+GAo1iFpd8ay521JFu6wqOK7IzJTamwK61qMcBgf9o/vFPB3nK08Diocd4cuhtC/NV472o8VpW2ArATwjsYL7T/q2mV8CWDPZdgxfDpfUzjv2Aycfsz1oG1FAsQVuKG6lToNBObQWfhZYkgXlprVi5S6wVUyyMPJQL8kztJndeYs/icXQ4Dzuhqqr8ilSsovnn9SjWRVk2kpB/A03prPHT0xMlAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/91e904cd731a1fc8a1f4aacfae9be19e/9e594/outlier-impact.avif 192w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/3124f/outlier-impact.avif 384w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/0187b/outlier-impact.avif 768w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/7799c/outlier-impact.avif 1152w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/35b77/outlier-impact.avif 1184w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/avif\"\n            /><source\n              srcset=\"/static/91e904cd731a1fc8a1f4aacfae9be19e/dce26/outlier-impact.webp 192w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/955be/outlier-impact.webp 384w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/58a3c/outlier-impact.webp 768w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/ccee1/outlier-impact.webp 1152w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/e1404/outlier-impact.webp 1184w\"\n              sizes=\"(max-width: 768px) 100vw, 768px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/91e904cd731a1fc8a1f4aacfae9be19e/4eff4/outlier-impact.png 192w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/0cf21/outlier-impact.png 384w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/2b7c5/outlier-impact.png 768w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/ab7b1/outlier-impact.png 1152w,\n/static/91e904cd731a1fc8a1f4aacfae9be19e/3735f/outlier-impact.png 1184w\"\n            sizes=\"(max-width: 768px) 100vw, 768px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/91e904cd731a1fc8a1f4aacfae9be19e/2b7c5/outlier-impact.png\"\n            alt=\"이상치가 회귀 직선에 미치는 영향\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n    </span></p>\n<p align=\"center\" style=\"color: #888; font-size: 13px;\"><em>빨간 별이 이상치 1개. 이상치 하나만으로 회귀 직선 전체가 크게 왜곡된다.</em></p>\n<p><a href=\"/ml/workflow/\">머신러닝 프로젝트 워크플로우</a>에서 EDA를 강조하는 이유 중 하나가 바로 이것이다. 데이터를 먼저 시각화해서 이상치를 파악하는 게 모델을 학습하기 전에 꼭 거쳐야 하는 단계다.</p>\n<h3>훈련 데이터 성능만 보는 착각</h3>\n<p><code class=\"language-text\">model.fit(X_train, y_train)</code> 후 <code class=\"language-text\">model.score(X_train, y_train)</code>을 보면 실제보다 좋게 나올 수 있다. 반드시 테스트 데이터(<code class=\"language-text\">X_test, y_test</code>)로 성능을 검증해야 한다. 항상 <code class=\"language-text\">train_test_split()</code>을 사용하고, <strong>테스트 성능 기준</strong>으로 모델을 평가하자.</p>\n<hr>\n<h2>마치며</h2>\n<p>선형 회귀는 단순하지만, 그 단순함이 강점이다. 어떤 특성이 예측에 얼마나 기여하는지 w 값만 봐도 직관적으로 파악할 수 있다. 블랙박스인 복잡한 모델들과 달리, 해석 가능성(interpretability)이 높다.</p>\n<p>다음 글에서는 “잔차를 최소화한다”는 말을 수학적으로 정확하게 정의하는 **비용 함수(Cost Function)**를 다룬다. 왜 잔차를 제곱하는지, 왜 평균을 취하는지, 이 이유를 이해하고 나면 이후 경사하강법과 신경망으로 이어지는 흐름이 자연스럽게 보이기 시작한다.</p>\n<h2>참고자료</h2>\n<ul>\n<li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\" target=\"_blank\" rel=\"noopener noreferrer\">Scikit-learn LinearRegression Documentation</a></li>\n<li><a href=\"https://www.coursera.org/specializations/machine-learning-introduction\" target=\"_blank\" rel=\"noopener noreferrer\">Andrew Ng — Machine Learning Specialization (Coursera)</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=nk2CQITm_eo\" target=\"_blank\" rel=\"noopener noreferrer\">StatQuest: Linear Regression, Clearly Explained (YouTube)</a></li>\n<li><a href=\"https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset\" target=\"_blank\" rel=\"noopener noreferrer\">California Housing Dataset</a></li>\n</ul>","fields":{"slug":"/ml/linear-regression/"},"frontmatter":{"title":"선형 회귀(Linear Regression): 예측 모델의 시작점","summary":"머신러닝에서 가장 기초적인 예측 모델인 선형 회귀의 원리를 처음부터 끝까지 이해한다. 가설 함수, 잔차, 정규 방정식, sklearn 실전 예제까지 한 번에 정리.","date":"2026.03.01.","rawDate":"2026-03-01","category":"Machine Learning","thumbnail":null}}}]}},"pageContext":{"slug":"/ml/linear-regression/"}},"staticQueryHashes":[],"slicesMap":{}}